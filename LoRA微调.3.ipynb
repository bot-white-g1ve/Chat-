{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.19\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ===Outcome of build_prompt=== \n",
      "[Round 1]\n",
      "\n",
      "问：AI是什么？\n",
      "\n",
      "答：\n",
      " ===Special Tokens=== \n",
      "BOS token ID: 1\n",
      "EOS token ID: 2\n",
      "PAD token ID: 0\n",
      "SOP token ID: 64792\n",
      "EOP token ID: 64793\n",
      "MASK token ID: 64789\n",
      "GMASK token ID: 64790\n",
      " ===Question ids using encode with special tokens=== \n",
      "[64790, 64792, 790, 30951, 517, 30910, 30939, 30996, 13, 13, 54761, 31211, 23833, 32664, 31514, 13, 13, 55437, 31211]\n",
      " ===Answer ids===\n",
      "[11265, 54532, 34797, 30946, 11868, 9596, 13067, 30945, 54530, 56068, 55172]\n"
     ]
    }
   ],
   "source": [
    "# 载入模型\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained('/ssd/xthu/ChatGLM2-6B/chatglm2-6b', trust_remote_code=True)\n",
    "\n",
    "prompt = tokenizer.build_prompt(\"AI是什么？\", None)\n",
    "print(\" ===Outcome of build_prompt=== \")\n",
    "print(prompt)\n",
    "\n",
    "print(\" ===Special Tokens=== \")\n",
    "print(\"BOS token ID:\", tokenizer.get_command(\"<bos>\"))\n",
    "print(\"EOS token ID:\", tokenizer.get_command(\"<eos>\"))\n",
    "print(\"PAD token ID:\", tokenizer.get_command(\"<pad>\"))\n",
    "print(\"SOP token ID:\", tokenizer.get_command(\"sop\"))\n",
    "print(\"EOP token ID:\", tokenizer.get_command(\"eop\"))\n",
    "print(\"MASK token ID:\", tokenizer.get_command(\"[MASK]\"))\n",
    "print(\"GMASK token ID:\", tokenizer.get_command(\"[gMASK]\"))\n",
    "\n",
    "max_source_length = 100\n",
    "max_target_length = 200\n",
    "q_ids = tokenizer.encode(text=prompt, add_special_tokens=True, truncation=True,\n",
    "                                      max_length=max_source_length)\n",
    "print(\" ===Question ids using encode with special tokens=== \")\n",
    "print(q_ids)\n",
    "a_ids = tokenizer.encode(text=\"AI是人工智能(Artificial Intelligence)的缩写\", add_special_tokens=False, truncation=True,\n",
    "                                      max_length=max_target_length)\n",
    "print(\" ===Answer ids===\")\n",
    "print(a_ids)\n",
    "\n",
    "# print(\" ===Question ids using tokenizer() without argument=== \")\n",
    "# q_ids2 = tokenizer([prompt])\n",
    "# print(q_ids2)\n",
    "# 结果是q_ids2的input_ids和q_ids一致，但是多了attention_mask, position_ids之类的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " !!!Loading Model!!! \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42ffe7acd52d4847b210c7d4e81d0df7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,949,696 || all params: 6,245,533,696 || trainable%: 0.0312\n"
     ]
    }
   ],
   "source": [
    "print(\" !!!Loading Model!!! \")\n",
    "import torch\n",
    "model = AutoModel.from_pretrained(\"/ssd/xthu/ChatGLM2-6B/chatglm2-6b\", trust_remote_code=True)\n",
    "max_source_length = 128\n",
    "max_target_length = 128\n",
    "epochs=5\n",
    "batch_size=10\n",
    "lr = 1e-6\n",
    "lora_r=8\n",
    "device=torch.device(\"cuda\")\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "peft_config=LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=lora_r,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1\n",
    ")\n",
    "model = get_peft_model(model, peft_config).to(device)\n",
    "model.print_trainable_parameters()\n",
    "# model=model.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用来清除内存（NON-USE）\n",
    "for param in model.parameters():\n",
    "    del param\n",
    "del model\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用来检测清除内存（NON-USE）\n",
    "print(torch.cuda.memory_allocated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " !!!Start Training!!! \n",
      " ===Query=== \n",
      "学猫叫\n",
      " ===Prompt=== \n",
      "[Round 1]\n",
      "\n",
      "问：学猫叫\n",
      "\n",
      "答：\n",
      " ===Inputs=== \n",
      "{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n",
      " 'input_ids': tensor([[64790, 64792,   790, 30951,   517, 30910, 30939, 30996,    13,    13,\n",
      "         54761, 31211, 54545, 56267, 55483,    13,    13, 55437, 31211]]),\n",
      " 'position_ids': tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "         18]])}\n"
     ]
    }
   ],
   "source": [
    "print(\" !!!Start Training!!! \")\n",
    "import time\n",
    "print(\" ===Query=== \")\n",
    "query = \"学猫叫\"\n",
    "print(query)\n",
    "print(\" ===Prompt=== \")\n",
    "prompt = tokenizer.build_prompt(query, None)\n",
    "print(prompt)\n",
    "print(\" ===Inputs=== \")\n",
    "inputs = tokenizer([prompt], return_tensors=\"pt\")\n",
    "from pprint import pprint\n",
    "pprint(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 试验：history长啥样（NON-USE）\n",
    "print(\"This is for DEBUGing\")\n",
    "model = model.eval()\n",
    "response, history = model.chat(tokenizer, \"你好\", history=[])\n",
    "print(\" === Response === \")\n",
    "print(response)\n",
    "print(\" === History === \")\n",
    "print(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实验：如果有history（NON-USE）\n",
    "query = \"学猫叫\"\n",
    "history = [(\"学狗叫\",\"汪汪汪\")]\n",
    "prompt = tokenizer.build_prompt(query, history)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试device（NON-USE）\n",
    "print(f\"model:{model.device}\")\n",
    "print(f\"inputs.input_ids:{inputs['input_ids'].device}\")\n",
    "print(f\"inputs.attention_mask:{inputs['attention_mask'].device}\")\n",
    "print(f\"inputs.position_ids:{inputs['position_ids'].device}\")\n",
    "print(f\"labels:{labels.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.generate()测试（NON-USE）\n",
    "# Default args of generating \n",
    "max_length=8192\n",
    "num_beams=1\n",
    "do_sample=True\n",
    "top_p=0.8\n",
    "temperature=0.8\n",
    "logits_processor=None\n",
    "gen_kwargs = {\"max_length\": max_length, \"num_beams\": num_beams, \"do_sample\": do_sample, \"top_p\": top_p,\n",
    "                      \"temperature\": temperature, \"logits_processor\": logits_processor}\n",
    "# Put inputs into cuda\n",
    "inputs['input_ids'] = inputs['input_ids'].to(device)\n",
    "inputs['attention_mask'] = inputs['attention_mask'].to(device)\n",
    "inputs['position_ids'] = inputs['position_ids'].to(device)\n",
    "\n",
    "print(\" ===Outputs=== \")\n",
    "outputs=model.generate(**inputs, **gen_kwargs)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 试验：output decode（NON-USE）\n",
    "outputs_list = outputs.tolist()[0]\n",
    "response = tokenizer.decode(outputs_list)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 试验：one token decode（NON-USE）\n",
    "str_item = tokenizer.decode([34211])\n",
    "print(str_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ===Labels=== \n",
      "label1:[[64790, 64792, 790, 30951, 517, 30910, 30939, 30996, 13, 13, 54761, 31211, 54545, 56267, 55483, 13, 13, 55437, 31211], 30910, 59000, 59000, 59000, 2]\n",
      "label2:[[64790, 64792, 790, 30951, 517, 30910, 30939, 30996, 13, 13, 54761, 31211, 54545, 56267, 55483, 13, 13, 55437, 31211], [30910, 59000, 59000, 59000], [2]]\n",
      "label:[[64790, 64792, 790, 30951, 517, 30910, 30939, 30996, 13, 13, 54761, 31211, 54545, 56267, 55483, 13, 13, 55437, 31211, 30910, 59000, 59000, 59000, 2]]\n",
      "labels:[[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 30910, 59000, 59000, 59000, 2]]\n",
      "labels.len:24\n"
     ]
    }
   ],
   "source": [
    "print(\" ===Labels=== \")\n",
    "query_ids = inputs[\"input_ids\"]\n",
    "answer_ids = tokenizer.encode(\"喵喵喵\", add_special_tokens=False)\n",
    "label1 = query_ids.tolist()+answer_ids+[2]\n",
    "print(f\"label1:{label1}\")\n",
    "label2 = query_ids.tolist()+[answer_ids]+[[2]]\n",
    "print(f\"label2:{label2}\")\n",
    "label_list = [query_ids.tolist()[0]+answer_ids+[2]]\n",
    "print(f\"label:{label_list}\")\n",
    "\n",
    "labels_list = [[-100]*len(query_ids.tolist()[0]) + answer_ids + [2]]\n",
    "print(f\"labels:{labels_list}\")\n",
    "print(f\"labels.len:{len(labels_list[0])}\")\n",
    "labels = torch.tensor(labels_list).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 试验：input_ids和label的维度（NON-USE）\n",
    "print(\" ===Size for input_ids and labels=== \")\n",
    "print(inputs[\"input_ids\"].size())\n",
    "print(label.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ===input_ids_list=== \n",
      "[[64790, 64792, 790, 30951, 517, 30910, 30939, 30996, 13, 13, 54761, 31211, 54545, 56267, 55483, 13, 13, 55437, 31211, 30910, 59000, 59000, 59000, 2]]\n",
      "len:24\n",
      " ===attention_mask_list=== \n",
      "[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]]\n",
      "len:24\n",
      " ===position_ids_list=== \n",
      "tensor([[1, 2, 3, 4, 5]])\n",
      "[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]]\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "input_ids_list=[query_ids.tolist()[0]+answer_ids+[2]]\n",
    "print(\" ===input_ids_list=== \")\n",
    "print(input_ids_list)\n",
    "print(f\"len:{len(input_ids_list[0])}\")\n",
    "input_ids = torch.tensor(input_ids_list).to(device)\n",
    "\n",
    "print(\" ===attention_mask_list=== \")\n",
    "attention_mask_list = inputs['attention_mask'].tolist()\n",
    "attention_mask_list = [attention_mask_list[0]+[0]*(len(answer_ids)+1)]\n",
    "print(attention_mask_list)\n",
    "print(f\"len:{len(attention_mask_list[0])}\")\n",
    "attention_mask = torch.tensor(attention_mask_list).to(device)\n",
    "\n",
    "print(\" ===position_ids_list=== \")\n",
    "position_ids_len = len(input_ids_list[0])\n",
    "position_ids1 = torch.arange(position_ids_len, dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "position_ids2_answer = torch.arange(1, len(answer_ids)+2, dtype=torch.long).unsqueeze(0)\n",
    "print(position_ids2_answer)\n",
    "position_ids2_padding = torch.zeros(len(query_ids.tolist()[0]), dtype=torch.long).unsqueeze(0)\n",
    "position_ids2 = torch.cat((position_ids2_padding, position_ids2_answer), dim=1)\n",
    "\n",
    "# position_ids = torch.cat((position_ids1, position_ids2), dim=0)\n",
    "# print(position_ids.tolist())\n",
    "# print(len(position_ids[0].tolist()))\n",
    "# print(len(position_ids[1].tolist()))\n",
    "# position_ids = position_ids.to(device)\n",
    "position_ids = position_ids1\n",
    "print(position_ids.tolist())\n",
    "print(len(position_ids[0].tolist()))\n",
    "position_ids = position_ids.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:cuda:0\n",
      "input_ids:cuda:0\n",
      "attention_mask:cuda:0\n",
      "position_ids:cuda:0\n",
      "labels:cuda:0\n"
     ]
    }
   ],
   "source": [
    "# 检查device（NON-USE）\n",
    "print(f\"model:{model.device}\")\n",
    "print(f\"input_ids:{input_ids.device}\")\n",
    "print(f\"attention_mask:{attention_mask.device}\")\n",
    "print(f\"position_ids:{position_ids.device}\")\n",
    "print(f\"labels:{labels.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(params=model.parameters(), lr=lr)\n",
    "optimizer.zero_grad()\n",
    "model = model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ===First step of training=== \n",
      "time used:1.208711862564087\n",
      "loss:1.7470703125\n",
      " ===Save the model=== \n",
      "the output dir is chatglm2-6b\n",
      " ===Save the optimizer=== \n",
      "the output path is chatglm2-6b/optimizer.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xthu/anaconda3/envs/py3.9/lib/python3.9/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /ssd/xthu/ChatGLM2-6B/chatglm2-6b - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "train_output = model(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    position_ids=position_ids,\n",
    "    labels=labels,\n",
    ")\n",
    "loss = train_output.loss\n",
    "loss.backward()\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "optimizer.step()\n",
    "optimizer.zero_grad()\n",
    "end_time = time.time()\n",
    "print(\" ===First step of training=== \")\n",
    "print(f\"time used:{end_time-start_time}\")\n",
    "print(f\"loss:{loss}\")\n",
    "\n",
    "model_output_dir = \"chatglm2-6b\"\n",
    "print(\" ===Save the model=== \")\n",
    "print(f\"the output dir is {model_output_dir}\")\n",
    "model.save_pretrained(model_output_dir)\n",
    "# LoRA will automatically save\n",
    "\n",
    "print(\" ===Save the optimizer=== \")\n",
    "import os\n",
    "optimizer_state_path = os.path.join(model_output_dir, \"optimizer.model\")\n",
    "print(f\"the output path is {optimizer_state_path}\")\n",
    "torch.save(optimizer.state_dict(), optimizer_state_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking input data for NaNs...\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(False, device='cuda:0')\n",
      "tensor(False, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# 检查输入中是否有nan\n",
    "print(\"Checking input data for NaNs...\")\n",
    "print(torch.isnan(input_ids).any())\n",
    "print(torch.isnan(attention_mask).any())\n",
    "print(torch.isnan(position_ids).any())\n",
    "print(torch.isnan(labels).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查模型结构（NON-USE）\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试：到底为什么有nan（NON-USE）\n",
    "start_time = time.time()\n",
    "train_output = model(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    position_ids=position_ids,\n",
    "    labels=labels,\n",
    ")\n",
    "loss = train_output.loss\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.7471, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "Random parameter data type: torch.float16\n"
     ]
    }
   ],
   "source": [
    "# 测试：到底为什么有nan（NON-USE）\n",
    "print(loss)\n",
    "\n",
    "params = list(model.parameters())\n",
    "import random\n",
    "random_param = random.choice(params)\n",
    "print(f'Random parameter data type: {random_param.dtype}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of parameter base_model.model.transformer.encoder.layers.0.self_attention.query_key_value.lora_A.default.weight: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16)\n",
      "Update for parameter base_model.model.transformer.encoder.layers.0.self_attention.query_key_value.lora_A.default.weight: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16)\n",
      "Gradient of parameter base_model.model.transformer.encoder.layers.0.self_attention.query_key_value.lora_B.default.weight: tensor([[ 3.2783e-05, -4.7970e-04,  9.1505e-04,  ..., -7.2241e-04,\n",
      "          7.1716e-04,  6.0415e-04],\n",
      "        [ 6.0797e-04,  3.9077e-04, -6.5470e-04,  ...,  9.0361e-04,\n",
      "         -8.6212e-04, -4.6277e-04],\n",
      "        [ 1.9717e-04,  8.8453e-04, -9.3555e-04,  ...,  1.0395e-03,\n",
      "         -1.1978e-03, -7.0715e-04],\n",
      "        ...,\n",
      "        [ 2.1698e-02, -3.0533e-02,  5.9937e-02,  ..., -4.5258e-02,\n",
      "         -8.8989e-02, -3.1403e-02],\n",
      "        [ 6.6490e-03, -4.0405e-02,  4.2798e-01,  ..., -3.0933e-01,\n",
      "         -2.6978e-01, -6.7322e-02],\n",
      "        [-9.3536e-03,  3.8544e-02, -1.0765e-02,  ...,  4.2664e-02,\n",
      "         -7.2876e-02, -7.7972e-03]], device='cuda:0', dtype=torch.float16)\n",
      "Update for parameter base_model.model.transformer.encoder.layers.0.self_attention.query_key_value.lora_B.default.weight: tensor([[ 0.0000e+00, -0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ...,  0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ...,  0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00],\n",
      "        ...,\n",
      "        [ 0.0000e+00, -5.9605e-08,  5.9605e-08,  ..., -5.9605e-08,\n",
      "         -5.9605e-08, -5.9605e-08],\n",
      "        [ 0.0000e+00, -5.9605e-08,  4.1723e-07,  ..., -2.9802e-07,\n",
      "         -2.9802e-07, -5.9605e-08],\n",
      "        [-0.0000e+00,  5.9605e-08, -0.0000e+00,  ...,  5.9605e-08,\n",
      "         -5.9605e-08, -0.0000e+00]], device='cuda:0', dtype=torch.float16)\n",
      "Gradient of parameter base_model.model.transformer.encoder.layers.1.self_attention.query_key_value.lora_A.default.weight: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16)\n",
      "Update for parameter base_model.model.transformer.encoder.layers.1.self_attention.query_key_value.lora_A.default.weight: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16)\n",
      "Gradient of parameter base_model.model.transformer.encoder.layers.1.self_attention.query_key_value.lora_B.default.weight: tensor([[-2.0294e-03,  4.9820e-03, -3.8004e-04,  ...,  1.8253e-03,\n",
      "         -2.2411e-05, -3.6144e-03],\n",
      "        [-7.3624e-04,  4.4847e-04, -2.6894e-03,  ..., -7.5073e-03,\n",
      "         -7.3013e-03, -9.3994e-03],\n",
      "        [ 5.9509e-04, -1.3618e-03,  1.7750e-04,  ..., -9.6989e-04,\n",
      "         -1.3077e-04,  7.9012e-04],\n",
      "        ...,\n",
      "        [-5.5206e-02,  7.4158e-02,  2.6749e-02,  ...,  3.2410e-02,\n",
      "          2.7557e-02, -2.6581e-02],\n",
      "        [ 1.0536e-02, -1.4984e-02, -1.1641e-04,  ..., -1.9226e-02,\n",
      "         -1.6052e-02,  1.0254e-02],\n",
      "        [ 3.6163e-02,  4.4342e-02,  7.0251e-02,  ..., -4.2725e-02,\n",
      "         -2.1985e-01,  5.2399e-02]], device='cuda:0', dtype=torch.float16)\n",
      "Update for parameter base_model.model.transformer.encoder.layers.1.self_attention.query_key_value.lora_B.default.weight: tensor([[-0.0000e+00,  0.0000e+00, -0.0000e+00,  ...,  0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00],\n",
      "        [-0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00],\n",
      "        [ 0.0000e+00, -0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
      "         -0.0000e+00,  0.0000e+00],\n",
      "        ...,\n",
      "        [-5.9605e-08,  5.9605e-08,  0.0000e+00,  ...,  5.9605e-08,\n",
      "          0.0000e+00, -0.0000e+00],\n",
      "        [ 0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
      "         -0.0000e+00,  0.0000e+00],\n",
      "        [ 5.9605e-08,  5.9605e-08,  5.9605e-08,  ..., -5.9605e-08,\n",
      "         -2.3842e-07,  5.9605e-08]], device='cuda:0', dtype=torch.float16)\n",
      "Gradient of parameter base_model.model.transformer.encoder.layers.2.self_attention.query_key_value.lora_A.default.weight: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16)\n",
      "Update for parameter base_model.model.transformer.encoder.layers.2.self_attention.query_key_value.lora_A.default.weight: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16)\n",
      "Gradient of parameter base_model.model.transformer.encoder.layers.2.self_attention.query_key_value.lora_B.default.weight: tensor([[-2.0111e-04, -1.1234e-03,  1.7443e-03,  ...,  5.7173e-04,\n",
      "          2.5406e-03, -7.7200e-04],\n",
      "        [ 1.5950e-04,  4.3082e-04,  5.1641e-04,  ..., -4.8637e-04,\n",
      "          9.6464e-04, -4.1986e-04],\n",
      "        [-1.4806e-04,  1.3578e-04, -3.1018e-04,  ...,  1.0145e-04,\n",
      "         -6.0415e-04,  3.5501e-04],\n",
      "        ...,\n",
      "        [ 8.8043e-03, -3.4027e-02,  1.2178e-03,  ...,  3.9581e-02,\n",
      "          1.4336e-02, -2.3155e-03],\n",
      "        [-7.5531e-03, -4.6631e-02,  2.5162e-02,  ...,  4.1138e-02,\n",
      "          5.2185e-02, -1.5900e-02],\n",
      "        [ 2.2522e-02,  9.0454e-02, -7.0435e-02,  ..., -4.8706e-02,\n",
      "         -1.1572e-01,  3.7079e-02]], device='cuda:0', dtype=torch.float16)\n",
      "Update for parameter base_model.model.transformer.encoder.layers.2.self_attention.query_key_value.lora_B.default.weight: tensor([[-0.0000e+00, -0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00, -0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
      "          0.0000e+00, -0.0000e+00],\n",
      "        [-0.0000e+00,  0.0000e+00, -0.0000e+00,  ...,  0.0000e+00,\n",
      "         -0.0000e+00,  0.0000e+00],\n",
      "        ...,\n",
      "        [ 0.0000e+00, -5.9605e-08,  0.0000e+00,  ...,  5.9605e-08,\n",
      "          0.0000e+00, -0.0000e+00],\n",
      "        [-0.0000e+00, -5.9605e-08,  0.0000e+00,  ...,  5.9605e-08,\n",
      "          5.9605e-08, -0.0000e+00],\n",
      "        [ 0.0000e+00,  1.1921e-07, -5.9605e-08,  ..., -5.9605e-08,\n",
      "         -1.1921e-07,  5.9605e-08]], device='cuda:0', dtype=torch.float16)\n",
      "Gradient of parameter base_model.model.transformer.encoder.layers.3.self_attention.query_key_value.lora_A.default.weight: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16)\n",
      "Update for parameter base_model.model.transformer.encoder.layers.3.self_attention.query_key_value.lora_A.default.weight: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16)\n",
      "Gradient of parameter base_model.model.transformer.encoder.layers.3.self_attention.query_key_value.lora_B.default.weight: tensor([[ 0.0054, -0.0022, -0.0004,  ..., -0.0019,  0.0018, -0.0029],\n",
      "        [-0.0002,  0.0007,  0.0004,  ..., -0.0003, -0.0009, -0.0011],\n",
      "        [-0.0154,  0.0044,  0.0005,  ...,  0.0059, -0.0052,  0.0081],\n",
      "        ...,\n",
      "        [-0.0862,  0.0408,  0.0312,  ...,  0.0170, -0.0294,  0.0017],\n",
      "        [ 0.0005, -0.0396, -0.0348,  ...,  0.0226,  0.0033, -0.0107],\n",
      "        [ 0.0547,  0.0030,  0.0034,  ..., -0.0155,  0.0021, -0.0679]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "Update for parameter base_model.model.transformer.encoder.layers.3.self_attention.query_key_value.lora_B.default.weight: tensor([[ 0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
      "          0.0000e+00, -0.0000e+00],\n",
      "        [-0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00],\n",
      "        [-0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "         -0.0000e+00,  0.0000e+00],\n",
      "        ...,\n",
      "        [-5.9605e-08,  5.9605e-08,  5.9605e-08,  ...,  0.0000e+00,\n",
      "         -0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00, -5.9605e-08, -5.9605e-08,  ...,  0.0000e+00,\n",
      "          0.0000e+00, -0.0000e+00],\n",
      "        [ 5.9605e-08,  0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
      "          0.0000e+00, -5.9605e-08]], device='cuda:0', dtype=torch.float16)\n",
      "Gradient of parameter base_model.model.transformer.encoder.layers.4.self_attention.query_key_value.lora_A.default.weight: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16)\n",
      "Update for parameter base_model.model.transformer.encoder.layers.4.self_attention.query_key_value.lora_A.default.weight: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16)\n",
      "Gradient of parameter base_model.model.transformer.encoder.layers.4.self_attention.query_key_value.lora_B.default.weight: tensor([[ 5.3787e-04,  6.8760e-04,  6.9809e-04,  ...,  1.5676e-05,\n",
      "         -1.2703e-03,  2.5768e-03],\n",
      "        [-2.4071e-03, -1.8063e-03, -4.3106e-03,  ..., -1.1759e-03,\n",
      "          1.9464e-03, -6.9141e-04],\n",
      "        [ 2.4452e-03,  3.4928e-04,  3.6106e-03,  ..., -1.1379e-04,\n",
      "         -1.4782e-04, -7.8678e-04],\n",
      "        ...,\n",
      "        [-5.9723e-02, -2.2186e-02, -7.0557e-02,  ..., -1.6586e-02,\n",
      "          1.2062e-02, -2.2476e-02],\n",
      "        [ 2.2751e-02,  4.4830e-02,  6.5063e-02,  ...,  6.2439e-02,\n",
      "         -6.6589e-02,  6.2408e-02],\n",
      "        [-1.0132e-02, -6.9092e-02, -7.8430e-02,  ..., -4.0161e-02,\n",
      "          1.0028e-01, -4.7760e-02]], device='cuda:0', dtype=torch.float16)\n",
      "Update for parameter base_model.model.transformer.encoder.layers.4.self_attention.query_key_value.lora_B.default.weight: tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "         -0.0000e+00,  0.0000e+00],\n",
      "        [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
      "          0.0000e+00, -0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00],\n",
      "        ...,\n",
      "        [-5.9605e-08, -0.0000e+00, -5.9605e-08,  ..., -0.0000e+00,\n",
      "          0.0000e+00, -0.0000e+00],\n",
      "        [ 0.0000e+00,  5.9605e-08,  5.9605e-08,  ...,  5.9605e-08,\n",
      "         -5.9605e-08,  5.9605e-08],\n",
      "        [-0.0000e+00, -5.9605e-08, -5.9605e-08,  ..., -5.9605e-08,\n",
      "          1.1921e-07, -5.9605e-08]], device='cuda:0', dtype=torch.float16)\n",
      "Gradient of parameter base_model.model.transformer.encoder.layers.5.self_attention.query_key_value.lora_A.default.weight: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16)\n",
      "Update for parameter base_model.model.transformer.encoder.layers.5.self_attention.query_key_value.lora_A.default.weight: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16)\n",
      "Gradient of parameter base_model.model.transformer.encoder.layers.5.self_attention.query_key_value.lora_B.default.weight: tensor([[ 1.0666e-02, -5.9395e-03,  2.7676e-03,  ..., -5.5313e-03,\n",
      "         -5.1498e-03,  2.4128e-03],\n",
      "        [-3.4981e-03,  5.8532e-05,  1.6813e-03,  ...,  1.5202e-03,\n",
      "          4.9934e-03, -9.9468e-04],\n",
      "        [ 3.4580e-03,  2.6941e-04, -5.5599e-04,  ..., -1.8110e-03,\n",
      "         -2.6779e-03,  2.5349e-03],\n",
      "        ...,\n",
      "        [ 3.8269e-02, -2.0767e-02, -1.1169e-02,  ..., -1.8784e-02,\n",
      "         -3.6163e-02, -1.7798e-04],\n",
      "        [ 1.0565e-01, -6.1920e-02, -7.1220e-03,  ..., -2.6123e-02,\n",
      "         -1.0992e-01,  5.4131e-03],\n",
      "        [ 6.1302e-03, -2.2995e-02, -1.3443e-02,  ..., -6.4583e-03,\n",
      "          5.5027e-04,  9.2545e-03]], device='cuda:0', dtype=torch.float16)\n",
      "Update for parameter base_model.model.transformer.encoder.layers.5.self_attention.query_key_value.lora_B.default.weight: tensor([[ 0.0000e+00, -0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
      "         -0.0000e+00,  0.0000e+00],\n",
      "        [-0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00, -0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
      "         -0.0000e+00,  0.0000e+00],\n",
      "        ...,\n",
      "        [ 5.9605e-08, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
      "         -5.9605e-08, -0.0000e+00],\n",
      "        [ 1.1921e-07, -5.9605e-08, -0.0000e+00,  ..., -0.0000e+00,\n",
      "         -1.1921e-07,  0.0000e+00],\n",
      "        [ 0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00]], device='cuda:0', dtype=torch.float16)\n",
      "Gradient of parameter base_model.model.transformer.encoder.layers.6.self_attention.query_key_value.lora_A.default.weight: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16)\n",
      "Update for parameter base_model.model.transformer.encoder.layers.6.self_attention.query_key_value.lora_A.default.weight: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16)\n",
      "Gradient of parameter base_model.model.transformer.encoder.layers.6.self_attention.query_key_value.lora_B.default.weight: tensor([[ 1.1081e-04, -8.7857e-05, -1.8978e-04,  ...,  3.4213e-05,\n",
      "         -1.6201e-04, -8.2195e-05],\n",
      "        [ 2.5654e-04, -1.4186e-04, -1.7965e-04,  ...,  1.7083e-04,\n",
      "         -2.5821e-04, -1.9109e-04],\n",
      "        [ 9.6750e-04, -2.1040e-04, -6.9761e-04,  ...,  3.5167e-04,\n",
      "         -8.7214e-04, -8.4066e-04],\n",
      "        ...,\n",
      "        [-1.5100e-01,  3.2654e-02,  1.0010e-01,  ..., -3.7018e-02,\n",
      "          1.6516e-01,  1.5039e-01],\n",
      "        [ 1.3660e-01,  1.2047e-02, -6.3904e-02,  ...,  3.4210e-02,\n",
      "         -1.4795e-01, -1.0693e-01],\n",
      "        [-3.8361e-02,  2.0355e-02,  3.1494e-02,  ..., -1.6083e-02,\n",
      "          5.8533e-02,  6.2683e-02]], device='cuda:0', dtype=torch.float16)\n",
      "Update for parameter base_model.model.transformer.encoder.layers.6.self_attention.query_key_value.lora_B.default.weight: tensor([[ 0.0000e+00, -0.0000e+00, -0.0000e+00,  ...,  0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00],\n",
      "        [ 0.0000e+00, -0.0000e+00, -0.0000e+00,  ...,  0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00],\n",
      "        [ 0.0000e+00, -0.0000e+00, -0.0000e+00,  ...,  0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00],\n",
      "        ...,\n",
      "        [-1.7881e-07,  5.9605e-08,  1.1921e-07,  ..., -5.9605e-08,\n",
      "          1.7881e-07,  1.7881e-07],\n",
      "        [ 1.1921e-07,  0.0000e+00, -5.9605e-08,  ...,  5.9605e-08,\n",
      "         -1.1921e-07, -1.1921e-07],\n",
      "        [-5.9605e-08,  0.0000e+00,  5.9605e-08,  ..., -0.0000e+00,\n",
      "          5.9605e-08,  5.9605e-08]], device='cuda:0', dtype=torch.float16)\n",
      "Gradient of parameter base_model.model.transformer.encoder.layers.7.self_attention.query_key_value.lora_A.default.weight: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16)\n",
      "Update for parameter base_model.model.transformer.encoder.layers.7.self_attention.query_key_value.lora_A.default.weight: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16)\n",
      "Gradient of parameter base_model.model.transformer.encoder.layers.7.self_attention.query_key_value.lora_B.default.weight: tensor([[ 6.7043e-04,  6.3133e-04,  4.0779e-03,  ...,  1.7586e-03,\n",
      "         -4.5753e-04,  2.3365e-03],\n",
      "        [-1.1492e-03, -1.0948e-03, -3.4542e-03,  ..., -1.8482e-03,\n",
      "          2.7418e-04, -1.0710e-03],\n",
      "        [ 3.8266e-05,  9.4295e-05,  6.9237e-04,  ...,  2.6655e-04,\n",
      "         -1.0121e-04,  5.9032e-04],\n",
      "        ...,\n",
      "        [-2.3056e-02, -2.9327e-02, -6.3354e-02,  ..., -2.3773e-02,\n",
      "          2.8549e-02, -5.6610e-02],\n",
      "        [ 1.7242e-02,  7.5264e-03,  2.6550e-02,  ...,  2.9564e-04,\n",
      "         -2.1164e-02,  1.2016e-02],\n",
      "        [ 1.8967e-02,  4.2725e-03,  3.8849e-02,  ..., -1.2535e-02,\n",
      "         -1.6144e-02, -1.7631e-04]], device='cuda:0', dtype=torch.float16)\n",
      "Update for parameter base_model.model.transformer.encoder.layers.7.self_attention.query_key_value.lora_B.default.weight: tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "         -0.0000e+00,  0.0000e+00],\n",
      "        [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
      "          0.0000e+00, -0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "         -0.0000e+00,  0.0000e+00],\n",
      "        ...,\n",
      "        [-0.0000e+00, -0.0000e+00, -5.9605e-08,  ..., -0.0000e+00,\n",
      "          0.0000e+00, -5.9605e-08],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "         -0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  5.9605e-08,  ..., -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00]], device='cuda:0', dtype=torch.float16)\n",
      "Gradient of parameter base_model.model.transformer.encoder.layers.8.self_attention.query_key_value.lora_A.default.weight: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16)\n",
      "Update for parameter base_model.model.transformer.encoder.layers.8.self_attention.query_key_value.lora_A.default.weight: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16)\n",
      "Gradient of parameter base_model.model.transformer.encoder.layers.8.self_attention.query_key_value.lora_B.default.weight: tensor([[-0.0009, -0.0032,  0.0007,  ..., -0.0002, -0.0014, -0.0038],\n",
      "        [ 0.0024,  0.0144, -0.0010,  ...,  0.0028,  0.0060,  0.0190],\n",
      "        [-0.0007, -0.0007, -0.0004,  ..., -0.0002, -0.0005, -0.0014],\n",
      "        ...,\n",
      "        [ 0.0030,  0.0312,  0.0005,  ...,  0.0383, -0.0045,  0.0787],\n",
      "        [-0.0568, -0.0734,  0.0302,  ..., -0.0334, -0.0382, -0.0528],\n",
      "        [ 0.0078,  0.0057,  0.0051,  ..., -0.0106,  0.0050, -0.0112]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "Update for parameter base_model.model.transformer.encoder.layers.8.self_attention.query_key_value.lora_B.default.weight: tensor([[-0.0000e+00, -0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00],\n",
      "        ...,\n",
      "        [ 0.0000e+00,  5.9605e-08,  0.0000e+00,  ...,  5.9605e-08,\n",
      "         -0.0000e+00,  5.9605e-08],\n",
      "        [-5.9605e-08, -5.9605e-08,  5.9605e-08,  ..., -5.9605e-08,\n",
      "         -5.9605e-08, -5.9605e-08],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
      "          0.0000e+00, -0.0000e+00]], device='cuda:0', dtype=torch.float16)\n",
      "Gradient of parameter base_model.model.transformer.encoder.layers.9.self_attention.query_key_value.lora_A.default.weight: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16)\n",
      "Update for parameter base_model.model.transformer.encoder.layers.9.self_attention.query_key_value.lora_A.default.weight: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16)\n",
      "Gradient of parameter base_model.model.transformer.encoder.layers.9.self_attention.query_key_value.lora_B.default.weight: tensor([[ 5.4777e-05, -1.5461e-04,  1.0300e-04,  ..., -8.1658e-06,\n",
      "          5.3048e-05,  5.5432e-06],\n",
      "        [-2.7275e-04, -1.3733e-04,  6.3181e-05,  ..., -4.2140e-05,\n",
      "          1.5652e-04,  2.1815e-04],\n",
      "        [-3.9530e-04, -2.2221e-04,  5.6863e-05,  ..., -5.9187e-05,\n",
      "          3.6895e-05,  1.5223e-04],\n",
      "        ...,\n",
      "        [-1.0277e-02, -1.4366e-02,  1.0612e-02,  ..., -9.7752e-04,\n",
      "          8.8501e-03,  1.6830e-02],\n",
      "        [-6.9641e-02, -4.8950e-02,  6.2370e-03,  ...,  2.5452e-02,\n",
      "         -2.0309e-02, -1.6117e-03],\n",
      "        [-5.1788e-02, -3.0945e-02,  1.1246e-02,  ...,  3.4393e-02,\n",
      "         -1.2436e-02,  1.8396e-03]], device='cuda:0', dtype=torch.float16)\n",
      "Update for parameter base_model.model.transformer.encoder.layers.9.self_attention.query_key_value.lora_B.default.weight: tensor([[ 0.0000e+00, -0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [-0.0000e+00, -0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [-0.0000e+00, -0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        ...,\n",
      "        [-0.0000e+00, -0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [-5.9605e-08, -5.9605e-08,  0.0000e+00,  ...,  0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00],\n",
      "        [-5.9605e-08, -5.9605e-08,  0.0000e+00,  ...,  5.9605e-08,\n",
      "         -0.0000e+00,  0.0000e+00]], device='cuda:0', dtype=torch.float16)\n",
      "Gradient of parameter base_model.model.transformer.encoder.layers.10.self_attention.query_key_value.lora_A.default.weight: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16)\n",
      "Update for parameter base_model.model.transformer.encoder.layers.10.self_attention.query_key_value.lora_A.default.weight: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16)\n",
      "Gradient of parameter base_model.model.transformer.encoder.layers.10.self_attention.query_key_value.lora_B.default.weight: tensor([[-9.1696e-04, -6.8817e-03, -1.3876e-03,  ...,  3.0458e-05,\n",
      "         -3.3188e-03,  4.5547e-03],\n",
      "        [-8.2970e-04, -4.4403e-03, -1.0567e-03,  ..., -2.2781e-04,\n",
      "         -1.7986e-03,  3.0746e-03],\n",
      "        [ 4.2272e-04,  1.7710e-03,  2.1625e-04,  ..., -1.4648e-03,\n",
      "          1.0252e-03, -3.5954e-04],\n",
      "        ...,\n",
      "        [-2.2564e-03, -3.6678e-03, -4.7760e-02,  ..., -3.4393e-02,\n",
      "         -5.6038e-03,  4.6692e-02],\n",
      "        [-1.1925e-02, -4.5853e-03, -2.7328e-02,  ...,  8.1665e-02,\n",
      "         -2.2602e-03, -5.0903e-02],\n",
      "        [ 5.1758e-02, -7.1472e-02,  2.6306e-02,  ..., -7.8796e-02,\n",
      "         -4.7058e-02,  8.6487e-02]], device='cuda:0', dtype=torch.float16)\n",
      "Update for parameter base_model.model.transformer.encoder.layers.10.self_attention.query_key_value.lora_B.default.weight: tensor([[-0.0000e+00, -0.0000e+00, -0.0000e+00,  ...,  0.0000e+00,\n",
      "         -0.0000e+00,  0.0000e+00],\n",
      "        [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
      "         -0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
      "          0.0000e+00, -0.0000e+00],\n",
      "        ...,\n",
      "        [-0.0000e+00, -0.0000e+00, -5.9605e-08,  ..., -5.9605e-08,\n",
      "         -0.0000e+00,  5.9605e-08],\n",
      "        [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ...,  5.9605e-08,\n",
      "         -0.0000e+00, -5.9605e-08],\n",
      "        [ 5.9605e-08, -5.9605e-08,  0.0000e+00,  ..., -5.9605e-08,\n",
      "         -5.9605e-08,  5.9605e-08]], device='cuda:0', dtype=torch.float16)\n",
      "Gradient of parameter base_model.model.transformer.encoder.layers.11.self_attention.query_key_value.lora_A.default.weight: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16)\n",
      "Update for parameter base_model.model.transformer.encoder.layers.11.self_attention.query_key_value.lora_A.default.weight: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16)\n",
      "Gradient of parameter base_model.model.transformer.encoder.layers.11.self_attention.query_key_value.lora_B.default.weight: tensor([[-1.4639e-04,  5.4407e-04,  1.1790e-04,  ..., -9.5606e-04,\n",
      "         -2.6550e-03, -2.8753e-04],\n",
      "        [ 5.5134e-05,  2.7084e-04,  4.5598e-05,  ..., -6.1464e-04,\n",
      "         -8.9741e-04, -2.2984e-04],\n",
      "        [ 5.6624e-06,  7.4744e-05,  1.5676e-04,  ..., -8.8811e-05,\n",
      "          2.7895e-04,  5.2035e-05],\n",
      "        ...,\n",
      "        [ 2.6474e-02,  7.1777e-02, -5.6953e-03,  ..., -4.8523e-02,\n",
      "         -3.9581e-02, -1.0399e-02],\n",
      "        [ 2.2369e-02,  6.9885e-02,  1.1121e-01,  ..., -1.1664e-01,\n",
      "         -4.9591e-02, -2.1530e-02],\n",
      "        [ 3.4302e-02,  1.6266e-02,  4.8981e-02,  ..., -6.9397e-02,\n",
      "         -2.1332e-02, -3.6392e-03]], device='cuda:0', dtype=torch.float16)\n",
      "Update for parameter base_model.model.transformer.encoder.layers.11.self_attention.query_key_value.lora_B.default.weight: tensor([[-0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        ...,\n",
      "        [ 0.0000e+00,  5.9605e-08, -0.0000e+00,  ..., -5.9605e-08,\n",
      "         -5.9605e-08, -0.0000e+00],\n",
      "        [ 0.0000e+00,  5.9605e-08,  1.1921e-07,  ..., -1.1921e-07,\n",
      "         -5.9605e-08, -0.0000e+00],\n",
      "        [ 5.9605e-08,  0.0000e+00,  5.9605e-08,  ..., -5.9605e-08,\n",
      "         -0.0000e+00, -0.0000e+00]], device='cuda:0', dtype=torch.float16)\n",
      "Gradient of parameter base_model.model.transformer.encoder.layers.12.self_attention.query_key_value.lora_A.default.weight: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16)\n",
      "Update for parameter base_model.model.transformer.encoder.layers.12.self_attention.query_key_value.lora_A.default.weight: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16)\n",
      "Gradient of parameter base_model.model.transformer.encoder.layers.12.self_attention.query_key_value.lora_B.default.weight: tensor([[ 2.3403e-03, -8.0538e-04, -2.6441e-04,  ...,  1.1482e-03,\n",
      "          5.2023e-04,  5.0640e-04],\n",
      "        [-5.6553e-04, -7.9584e-04,  1.4324e-03,  ..., -1.7166e-04,\n",
      "          7.5769e-04, -1.1940e-03],\n",
      "        [-8.1396e-04,  5.3048e-06,  5.1546e-04,  ..., -4.1938e-04,\n",
      "         -6.6340e-05, -3.5119e-04],\n",
      "        ...,\n",
      "        [ 3.3173e-02,  2.2644e-02,  6.1073e-03,  ..., -6.2943e-03,\n",
      "          2.1194e-02,  6.4697e-02],\n",
      "        [-2.5711e-02,  2.0493e-02,  2.7512e-02,  ..., -1.9836e-02,\n",
      "         -1.5244e-02, -2.4017e-02],\n",
      "        [ 3.7750e-02,  3.7079e-03, -2.0844e-02,  ...,  3.2593e-02,\n",
      "         -1.8110e-03,  2.4109e-02]], device='cuda:0', dtype=torch.float16)\n",
      "Update for parameter base_model.model.transformer.encoder.layers.12.self_attention.query_key_value.lora_B.default.weight: tensor([[0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [-0.0000e+00, -0.0000e+00, 0.0000e+00,  ..., -0.0000e+00, 0.0000e+00,\n",
      "         -0.0000e+00],\n",
      "        [-0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., -0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00],\n",
      "        ...,\n",
      "        [5.9605e-08, 0.0000e+00, 0.0000e+00,  ..., -0.0000e+00, 0.0000e+00,\n",
      "         5.9605e-08],\n",
      "        [-0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., -0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00],\n",
      "        [5.9605e-08, 0.0000e+00, -0.0000e+00,  ..., 5.9605e-08, -0.0000e+00,\n",
      "         0.0000e+00]], device='cuda:0', dtype=torch.float16)\n",
      "Gradient of parameter base_model.model.transformer.encoder.layers.13.self_attention.query_key_value.lora_A.default.weight: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16)\n",
      "Update for parameter base_model.model.transformer.encoder.layers.13.self_attention.query_key_value.lora_A.default.weight: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16)\n",
      "Gradient of parameter base_model.model.transformer.encoder.layers.13.self_attention.query_key_value.lora_B.default.weight: tensor([[-1.2994e-04, -4.8685e-04, -1.5366e-04,  ..., -6.4421e-04,\n",
      "          1.0023e-03,  1.6403e-03],\n",
      "        [-1.7631e-04, -6.1703e-04, -2.8586e-04,  ..., -6.2656e-04,\n",
      "          5.4979e-04,  1.4515e-03],\n",
      "        [ 1.0312e-05, -2.5678e-04,  4.4167e-05,  ...,  3.2163e-04,\n",
      "          2.4724e-04,  1.2183e-04],\n",
      "        ...,\n",
      "        [ 5.3253e-02,  1.0857e-02,  3.2623e-02,  ...,  1.1041e-01,\n",
      "         -7.3303e-02, -9.1614e-02],\n",
      "        [ 1.0773e-01,  5.8441e-02,  6.3782e-02,  ...,  1.4246e-01,\n",
      "         -1.2085e-01, -1.4429e-01],\n",
      "        [ 9.4666e-02,  1.0910e-02,  4.4189e-02,  ...,  1.6992e-01,\n",
      "         -1.1188e-01, -1.4575e-01]], device='cuda:0', dtype=torch.float16)\n",
      "Update for parameter base_model.model.transformer.encoder.layers.13.self_attention.query_key_value.lora_B.default.weight: tensor([[-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00, -0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        ...,\n",
      "        [ 5.9605e-08,  0.0000e+00,  5.9605e-08,  ...,  1.1921e-07,\n",
      "         -5.9605e-08, -1.1921e-07],\n",
      "        [ 1.1921e-07,  5.9605e-08,  5.9605e-08,  ...,  1.1921e-07,\n",
      "         -1.1921e-07, -1.1921e-07],\n",
      "        [ 1.1921e-07,  0.0000e+00,  5.9605e-08,  ...,  1.7881e-07,\n",
      "         -1.1921e-07, -1.1921e-07]], device='cuda:0', dtype=torch.float16)\n",
      "Gradient of parameter base_model.model.transformer.encoder.layers.14.self_attention.query_key_value.lora_A.default.weight: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16)\n",
      "Update for parameter base_model.model.transformer.encoder.layers.14.self_attention.query_key_value.lora_A.default.weight: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16)\n",
      "Gradient of parameter base_model.model.transformer.encoder.layers.14.self_attention.query_key_value.lora_B.default.weight: tensor([[-6.0201e-06, -7.1764e-05,  1.9121e-04,  ...,  1.5080e-04,\n",
      "         -9.4473e-05,  1.5211e-04],\n",
      "        [-3.2723e-05,  1.2457e-04,  2.6870e-04,  ..., -1.4126e-04,\n",
      "          4.1604e-05,  7.1704e-05],\n",
      "        [ 1.4436e-04,  3.5882e-04, -8.4686e-04,  ..., -2.0909e-04,\n",
      "          3.7408e-04, -6.3515e-04],\n",
      "        ...,\n",
      "        [ 1.1559e-02, -2.8412e-02,  6.5857e-02,  ...,  4.5746e-02,\n",
      "         -5.1605e-02, -9.1171e-03],\n",
      "        [-4.9194e-02, -6.8787e-02,  9.3460e-03,  ...,  2.2293e-02,\n",
      "          8.9417e-03, -7.1899e-02],\n",
      "        [-1.7807e-02, -3.9642e-02,  3.5950e-02,  ...,  2.3849e-02,\n",
      "         -1.6083e-02, -2.5558e-02]], device='cuda:0', dtype=torch.float16)\n",
      "Update for parameter base_model.model.transformer.encoder.layers.14.self_attention.query_key_value.lora_B.default.weight: tensor([[-0.0000e+00, -0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "         -0.0000e+00,  0.0000e+00],\n",
      "        [-0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
      "          0.0000e+00, -0.0000e+00],\n",
      "        ...,\n",
      "        [ 0.0000e+00, -0.0000e+00,  5.9605e-08,  ...,  5.9605e-08,\n",
      "         -5.9605e-08, -0.0000e+00],\n",
      "        [-5.9605e-08, -5.9605e-08,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00, -5.9605e-08],\n",
      "        [-0.0000e+00, -5.9605e-08,  5.9605e-08,  ...,  0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00]], device='cuda:0', dtype=torch.float16)\n",
      "Gradient of parameter base_model.model.transformer.encoder.layers.15.self_attention.query_key_value.lora_A.default.weight: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16)\n",
      "Update for parameter base_model.model.transformer.encoder.layers.15.self_attention.query_key_value.lora_A.default.weight: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16)\n",
      "Gradient of parameter base_model.model.transformer.encoder.layers.15.self_attention.query_key_value.lora_B.default.weight: tensor([[-5.6624e-06,  3.0017e-04, -9.0122e-05,  ..., -4.6253e-05,\n",
      "          1.5199e-04,  2.5749e-04],\n",
      "        [ 1.7798e-04,  2.7156e-04,  2.1517e-04,  ..., -5.4896e-05,\n",
      "         -4.0221e-04,  3.4618e-04],\n",
      "        [ 2.6226e-05,  4.4763e-05,  2.9564e-05,  ..., -1.6987e-05,\n",
      "         -2.3365e-05,  2.4319e-05],\n",
      "        ...,\n",
      "        [-2.1896e-02,  4.2297e-02, -2.8046e-02,  ..., -7.5264e-03,\n",
      "          6.2012e-02,  5.0888e-03],\n",
      "        [ 5.5420e-02, -3.1372e-02,  8.3466e-03,  ..., -1.4664e-02,\n",
      "         -6.7261e-02, -4.8676e-02],\n",
      "        [-3.8635e-02,  2.2537e-02, -2.3346e-03,  ...,  9.7885e-03,\n",
      "          5.2917e-02,  6.1249e-02]], device='cuda:0', dtype=torch.float16)\n",
      "Update for parameter base_model.model.transformer.encoder.layers.15.self_attention.query_key_value.lora_B.default.weight: tensor([[-0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
      "         -0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
      "         -0.0000e+00,  0.0000e+00],\n",
      "        ...,\n",
      "        [-0.0000e+00,  5.9605e-08, -0.0000e+00,  ..., -0.0000e+00,\n",
      "          5.9605e-08,  0.0000e+00],\n",
      "        [ 5.9605e-08, -5.9605e-08,  0.0000e+00,  ..., -0.0000e+00,\n",
      "         -5.9605e-08, -5.9605e-08],\n",
      "        [-5.9605e-08,  0.0000e+00, -0.0000e+00,  ...,  0.0000e+00,\n",
      "          5.9605e-08,  5.9605e-08]], device='cuda:0', dtype=torch.float16)\n",
      "Gradient of parameter base_model.model.transformer.encoder.layers.16.self_attention.query_key_value.lora_A.default.weight: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16)\n",
      "Update for parameter base_model.model.transformer.encoder.layers.16.self_attention.query_key_value.lora_A.default.weight: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16)\n",
      "Gradient of parameter base_model.model.transformer.encoder.layers.16.self_attention.query_key_value.lora_B.default.weight: tensor([[ 4.2987e-04,  6.3241e-05,  2.0266e-04,  ...,  1.2398e-05,\n",
      "         -9.1136e-05, -2.0659e-04],\n",
      "        [-4.4703e-05,  4.3035e-05, -1.3602e-04,  ..., -1.3947e-05,\n",
      "         -1.2136e-04,  5.8353e-05],\n",
      "        [ 4.0460e-04,  3.1042e-04, -2.0850e-04,  ..., -2.2912e-04,\n",
      "         -8.1778e-04, -9.3222e-04],\n",
      "        ...,\n",
      "        [ 6.6467e-02, -1.9714e-02, -1.5823e-02,  ..., -1.7288e-02,\n",
      "         -2.8946e-02, -6.3416e-02],\n",
      "        [ 3.4973e-02, -3.2806e-02,  1.2035e-03,  ..., -1.1391e-02,\n",
      "         -4.2145e-02, -3.9276e-02],\n",
      "        [-1.3634e-02, -9.9335e-03,  1.4595e-02,  ...,  2.0645e-02,\n",
      "         -1.0368e-02,  7.8659e-03]], device='cuda:0', dtype=torch.float16)\n",
      "Update for parameter base_model.model.transformer.encoder.layers.16.self_attention.query_key_value.lora_B.default.weight: tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00],\n",
      "        [-0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
      "         -0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00],\n",
      "        ...,\n",
      "        [ 5.9605e-08, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
      "         -0.0000e+00, -5.9605e-08],\n",
      "        [ 5.9605e-08, -5.9605e-08,  0.0000e+00,  ..., -0.0000e+00,\n",
      "         -5.9605e-08, -5.9605e-08],\n",
      "        [-0.0000e+00, -0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "         -0.0000e+00,  0.0000e+00]], device='cuda:0', dtype=torch.float16)\n",
      "Gradient of parameter base_model.model.transformer.encoder.layers.17.self_attention.query_key_value.lora_A.default.weight: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16)\n",
      "Update for parameter base_model.model.transformer.encoder.layers.17.self_attention.query_key_value.lora_A.default.weight: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16)\n",
      "Gradient of parameter base_model.model.transformer.encoder.layers.17.self_attention.query_key_value.lora_B.default.weight: tensor([[ 1.1426e-04, -7.0393e-05,  4.1008e-04,  ...,  1.8239e-05,\n",
      "          1.5688e-04, -9.8884e-05],\n",
      "        [-1.5152e-04, -5.1200e-05, -1.1337e-04,  ...,  2.8431e-05,\n",
      "         -2.4259e-05,  9.2387e-06],\n",
      "        [ 2.8992e-04, -7.6056e-05,  3.6740e-04,  ..., -1.3614e-04,\n",
      "          2.1982e-04, -1.4436e-04],\n",
      "        ...,\n",
      "        [ 6.2744e-02, -2.9602e-02,  3.7403e-03,  ..., -3.3966e-02,\n",
      "          8.3191e-02, -5.6519e-02],\n",
      "        [-2.2964e-02,  8.4152e-03,  2.6417e-03,  ...,  3.3360e-03,\n",
      "         -4.6204e-02,  8.5754e-03],\n",
      "        [ 1.4963e-03,  3.9215e-03,  1.1772e-02,  ...,  1.9779e-03,\n",
      "          1.0246e-02,  6.3171e-03]], device='cuda:0', dtype=torch.float16)\n",
      "Update for parameter base_model.model.transformer.encoder.layers.17.self_attention.query_key_value.lora_B.default.weight: tensor([[ 0.0000e+00, -0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00, -0.0000e+00],\n",
      "        [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ...,  0.0000e+00,\n",
      "         -0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00, -0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
      "          0.0000e+00, -0.0000e+00],\n",
      "        ...,\n",
      "        [ 5.9605e-08, -0.0000e+00,  0.0000e+00,  ..., -5.9605e-08,\n",
      "          5.9605e-08, -5.9605e-08],\n",
      "        [-0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "         -5.9605e-08,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00]], device='cuda:0', dtype=torch.float16)\n",
      "Gradient of parameter base_model.model.transformer.encoder.layers.18.self_attention.query_key_value.lora_A.default.weight: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16)\n",
      "Update for parameter base_model.model.transformer.encoder.layers.18.self_attention.query_key_value.lora_A.default.weight: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16)\n",
      "Gradient of parameter base_model.model.transformer.encoder.layers.18.self_attention.query_key_value.lora_B.default.weight: tensor([[-2.5678e-04, -1.1486e-04,  3.1013e-03,  ...,  1.7223e-03,\n",
      "          3.8290e-04,  6.3896e-04],\n",
      "        [-7.0143e-04, -3.8433e-04,  3.9062e-03,  ...,  1.7071e-03,\n",
      "          9.6273e-04,  9.9850e-04],\n",
      "        [ 2.6059e-04,  9.2268e-05, -1.1837e-04,  ...,  1.2481e-04,\n",
      "         -3.3092e-04, -1.1605e-04],\n",
      "        ...,\n",
      "        [-1.8835e-03, -6.2828e-03, -2.9999e-02,  ..., -8.4915e-03,\n",
      "         -1.6317e-03, -9.9030e-03],\n",
      "        [-4.1428e-03,  2.2793e-03,  1.5373e-02,  ..., -1.8585e-02,\n",
      "         -1.9894e-03,  1.4183e-02],\n",
      "        [ 1.9522e-03, -1.2608e-03, -3.6621e-02,  ..., -5.6030e-02,\n",
      "         -7.6523e-03, -1.2445e-03]], device='cuda:0', dtype=torch.float16)\n",
      "Update for parameter base_model.model.transformer.encoder.layers.18.self_attention.query_key_value.lora_B.default.weight: tensor([[-0.0000e+00, -0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [-0.0000e+00, -0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ...,  0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00],\n",
      "        ...,\n",
      "        [-0.0000e+00, -0.0000e+00, -5.9605e-08,  ..., -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00],\n",
      "        [-0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
      "         -0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00, -0.0000e+00, -5.9605e-08,  ..., -5.9605e-08,\n",
      "         -0.0000e+00, -0.0000e+00]], device='cuda:0', dtype=torch.float16)\n",
      "Gradient of parameter base_model.model.transformer.encoder.layers.19.self_attention.query_key_value.lora_A.default.weight: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16)\n",
      "Update for parameter base_model.model.transformer.encoder.layers.19.self_attention.query_key_value.lora_A.default.weight: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16)\n",
      "Gradient of parameter base_model.model.transformer.encoder.layers.19.self_attention.query_key_value.lora_B.default.weight: tensor([[ 5.1856e-06, -2.8372e-05,  8.2433e-05,  ...,  2.0385e-05,\n",
      "         -1.3769e-05, -3.6538e-05],\n",
      "        [ 6.1035e-05,  2.3246e-06,  1.2469e-04,  ...,  5.1737e-05,\n",
      "         -7.0095e-05,  1.2589e-04],\n",
      "        [ 4.3511e-05,  1.7345e-05,  1.0848e-04,  ...,  6.7234e-05,\n",
      "         -4.2379e-05,  1.5199e-04],\n",
      "        ...,\n",
      "        [-1.6907e-02, -1.1253e-02, -2.7283e-02,  ..., -1.3756e-02,\n",
      "         -4.4174e-03, -3.5461e-02],\n",
      "        [ 1.1864e-02, -8.9951e-03, -1.2245e-02,  ..., -3.0487e-02,\n",
      "          6.9885e-03, -1.6815e-02],\n",
      "        [ 1.6861e-02, -7.1335e-03, -1.5511e-02,  ..., -1.4740e-02,\n",
      "          1.9264e-03, -1.8967e-02]], device='cuda:0', dtype=torch.float16)\n",
      "Update for parameter base_model.model.transformer.encoder.layers.19.self_attention.query_key_value.lora_B.default.weight: tensor([[ 0.0000e+00, -0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "         -0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "         -0.0000e+00,  0.0000e+00],\n",
      "        ...,\n",
      "        [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
      "         -0.0000e+00, -5.9605e-08],\n",
      "        [ 0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -5.9605e-08,\n",
      "          0.0000e+00, -0.0000e+00],\n",
      "        [ 0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
      "          0.0000e+00, -0.0000e+00]], device='cuda:0', dtype=torch.float16)\n",
      "Gradient of parameter base_model.model.transformer.encoder.layers.20.self_attention.query_key_value.lora_A.default.weight: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16)\n",
      "Update for parameter base_model.model.transformer.encoder.layers.20.self_attention.query_key_value.lora_A.default.weight: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16)\n",
      "Gradient of parameter base_model.model.transformer.encoder.layers.20.self_attention.query_key_value.lora_B.default.weight: tensor([[-1.0133e-06, -1.9550e-05, -1.0729e-06,  ...,  1.3113e-06,\n",
      "         -4.3511e-06, -2.9981e-05],\n",
      "        [ 3.2067e-05,  1.6868e-05,  1.8477e-06,  ...,  3.5822e-05,\n",
      "         -3.8922e-05, -3.5167e-05],\n",
      "        [-1.0610e-05, -3.4690e-05,  1.0073e-05,  ..., -2.6226e-06,\n",
      "         -6.5565e-07, -1.7285e-05],\n",
      "        ...,\n",
      "        [ 1.5961e-02, -2.5349e-03, -5.2757e-03,  ...,  2.6283e-03,\n",
      "          3.8548e-03,  7.8278e-03],\n",
      "        [ 6.0921e-03, -3.6831e-03,  5.7564e-03,  ..., -4.7989e-03,\n",
      "          2.5730e-03,  1.2932e-02],\n",
      "        [-2.1683e-02,  1.4076e-02, -1.3100e-02,  ...,  1.1002e-02,\n",
      "         -6.7329e-03, -2.1622e-02]], device='cuda:0', dtype=torch.float16)\n",
      "Update for parameter base_model.model.transformer.encoder.layers.20.self_attention.query_key_value.lora_B.default.weight: tensor([[-0., -0., -0.,  ..., 0., -0., -0.],\n",
      "        [0., 0., 0.,  ..., 0., -0., -0.],\n",
      "        [-0., -0., 0.,  ..., -0., -0., -0.],\n",
      "        ...,\n",
      "        [0., -0., -0.,  ..., 0., 0., 0.],\n",
      "        [0., -0., 0.,  ..., -0., 0., 0.],\n",
      "        [-0., 0., -0.,  ..., 0., -0., -0.]], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "Gradient of parameter base_model.model.transformer.encoder.layers.21.self_attention.query_key_value.lora_A.default.weight: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16)\n",
      "Update for parameter base_model.model.transformer.encoder.layers.21.self_attention.query_key_value.lora_A.default.weight: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16)\n",
      "Gradient of parameter base_model.model.transformer.encoder.layers.21.self_attention.query_key_value.lora_B.default.weight: tensor([[-1.8680e-04,  3.9959e-04, -2.2006e-04,  ..., -9.8515e-04,\n",
      "          2.5010e-04, -3.7599e-04],\n",
      "        [-3.9887e-04,  6.2180e-04, -2.6274e-04,  ..., -8.4114e-04,\n",
      "          6.0856e-05, -1.0937e-04],\n",
      "        [-8.1658e-06,  4.4417e-04,  4.3631e-04,  ...,  2.0504e-05,\n",
      "          8.6975e-04,  6.3896e-04],\n",
      "        ...,\n",
      "        [-1.8646e-02,  1.4580e-02, -1.9951e-03,  ..., -2.5497e-02,\n",
      "         -1.6113e-02,  3.1757e-03],\n",
      "        [ 6.8550e-03, -1.8906e-02,  1.9855e-03,  ...,  1.7120e-02,\n",
      "          2.5482e-02, -3.2692e-03],\n",
      "        [-3.8147e-02,  4.6143e-02,  1.6266e-02,  ..., -7.1106e-02,\n",
      "         -6.1066e-02, -1.4328e-02]], device='cuda:0', dtype=torch.float16)\n",
      "Update for parameter base_model.model.transformer.encoder.layers.21.self_attention.query_key_value.lora_B.default.weight: tensor([[-0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
      "          0.0000e+00, -0.0000e+00],\n",
      "        [-0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
      "          0.0000e+00, -0.0000e+00],\n",
      "        [-0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        ...,\n",
      "        [-0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
      "         -0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00, -0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00, -0.0000e+00],\n",
      "        [-5.9605e-08,  5.9605e-08,  0.0000e+00,  ..., -5.9605e-08,\n",
      "         -5.9605e-08, -0.0000e+00]], device='cuda:0', dtype=torch.float16)\n",
      "Gradient of parameter base_model.model.transformer.encoder.layers.22.self_attention.query_key_value.lora_A.default.weight: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16)\n",
      "Update for parameter base_model.model.transformer.encoder.layers.22.self_attention.query_key_value.lora_A.default.weight: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16)\n",
      "Gradient of parameter base_model.model.transformer.encoder.layers.22.self_attention.query_key_value.lora_B.default.weight: tensor([[ 1.3816e-04, -1.5783e-04, -1.3530e-04,  ..., -5.0545e-04,\n",
      "          2.3317e-04,  5.1975e-04],\n",
      "        [ 1.0413e-04,  9.8526e-05,  4.8184e-04,  ...,  1.6546e-04,\n",
      "          8.3804e-05, -2.2674e-04],\n",
      "        [-2.3615e-04,  8.0824e-05, -1.2165e-04,  ...,  3.5381e-04,\n",
      "         -2.6345e-04, -3.6430e-04],\n",
      "        ...,\n",
      "        [ 2.4529e-03, -4.1046e-03, -1.3031e-02,  ...,  2.2659e-02,\n",
      "          8.6594e-03, -3.6049e-03],\n",
      "        [-3.5076e-03, -1.3533e-03, -6.1760e-03,  ..., -7.6828e-03,\n",
      "         -3.9482e-03,  2.2488e-03],\n",
      "        [ 3.4866e-03,  4.7989e-03,  4.0321e-03,  ..., -6.7024e-03,\n",
      "         -5.8708e-03,  2.8286e-03]], device='cuda:0', dtype=torch.float16)\n",
      "Update for parameter base_model.model.transformer.encoder.layers.22.self_attention.query_key_value.lora_B.default.weight: tensor([[0., -0., -0.,  ..., -0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., -0.],\n",
      "        [-0., 0., -0.,  ..., 0., -0., -0.],\n",
      "        ...,\n",
      "        [0., -0., -0.,  ..., 0., 0., -0.],\n",
      "        [-0., -0., -0.,  ..., -0., -0., 0.],\n",
      "        [0., 0., 0.,  ..., -0., -0., 0.]], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "Gradient of parameter base_model.model.transformer.encoder.layers.23.self_attention.query_key_value.lora_A.default.weight: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16)\n",
      "Update for parameter base_model.model.transformer.encoder.layers.23.self_attention.query_key_value.lora_A.default.weight: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16)\n",
      "Gradient of parameter base_model.model.transformer.encoder.layers.23.self_attention.query_key_value.lora_B.default.weight: tensor([[-6.7830e-05, -3.8457e-04,  1.4365e-04,  ...,  1.9896e-04,\n",
      "          1.3137e-04,  1.0371e-05],\n",
      "        [ 2.1636e-05, -5.7697e-04,  4.9448e-04,  ...,  3.2353e-04,\n",
      "         -2.0683e-05, -5.7340e-05],\n",
      "        [ 2.2411e-05,  4.3797e-04, -3.4952e-04,  ..., -1.9419e-04,\n",
      "          1.5855e-04,  4.5538e-05],\n",
      "        ...,\n",
      "        [ 2.3331e-02,  4.2000e-03, -8.7585e-03,  ..., -1.3840e-02,\n",
      "         -2.6062e-02,  2.0233e-02],\n",
      "        [ 2.8038e-03, -7.5722e-04, -5.0011e-03,  ..., -1.8167e-03,\n",
      "         -5.3520e-03,  1.1911e-03],\n",
      "        [ 5.9814e-03, -5.1165e-04, -1.4046e-02,  ..., -1.1826e-02,\n",
      "         -1.1703e-02,  4.3678e-03]], device='cuda:0', dtype=torch.float16)\n",
      "Update for parameter base_model.model.transformer.encoder.layers.23.self_attention.query_key_value.lora_B.default.weight: tensor([[-0., -0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., -0., 0.,  ..., 0., -0., -0.],\n",
      "        [0., 0., -0.,  ..., -0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., -0.,  ..., -0., -0., 0.],\n",
      "        [0., -0., -0.,  ..., -0., -0., 0.],\n",
      "        [0., -0., -0.,  ..., -0., -0., 0.]], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "Gradient of parameter base_model.model.transformer.encoder.layers.24.self_attention.query_key_value.lora_A.default.weight: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16)\n",
      "Update for parameter base_model.model.transformer.encoder.layers.24.self_attention.query_key_value.lora_A.default.weight: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16)\n",
      "Gradient of parameter base_model.model.transformer.encoder.layers.24.self_attention.query_key_value.lora_B.default.weight: tensor([[-7.7486e-06,  1.5795e-04, -1.7405e-05,  ..., -2.0254e-04,\n",
      "         -2.8563e-04, -2.2423e-04],\n",
      "        [ 8.5056e-05, -1.0902e-04, -1.5306e-04,  ...,  3.1757e-04,\n",
      "          2.0719e-04,  3.0708e-04],\n",
      "        [-3.8719e-04, -7.3910e-06,  8.8036e-05,  ..., -1.6868e-04,\n",
      "          2.0456e-04, -1.0473e-04],\n",
      "        ...,\n",
      "        [ 9.1600e-04, -2.6264e-03,  2.9011e-03,  ...,  5.5981e-04,\n",
      "          7.7972e-03,  5.9052e-03],\n",
      "        [-6.0806e-03,  1.5860e-03, -1.9321e-03,  ...,  9.5978e-03,\n",
      "          1.7900e-03, -1.1024e-02],\n",
      "        [-1.6117e-03, -1.8635e-03,  5.6343e-03,  ..., -5.4855e-03,\n",
      "          7.4425e-03,  1.6663e-02]], device='cuda:0', dtype=torch.float16)\n",
      "Update for parameter base_model.model.transformer.encoder.layers.24.self_attention.query_key_value.lora_B.default.weight: tensor([[-0., 0., -0.,  ..., -0., -0., -0.],\n",
      "        [0., -0., -0.,  ..., 0., 0., 0.],\n",
      "        [-0., -0., 0.,  ..., -0., 0., -0.],\n",
      "        ...,\n",
      "        [0., -0., 0.,  ..., 0., 0., 0.],\n",
      "        [-0., 0., -0.,  ..., 0., 0., -0.],\n",
      "        [-0., -0., 0.,  ..., -0., 0., 0.]], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "Gradient of parameter base_model.model.transformer.encoder.layers.25.self_attention.query_key_value.lora_A.default.weight: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16)\n",
      "Update for parameter base_model.model.transformer.encoder.layers.25.self_attention.query_key_value.lora_A.default.weight: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16)\n",
      "Gradient of parameter base_model.model.transformer.encoder.layers.25.self_attention.query_key_value.lora_B.default.weight: tensor([[ 9.8348e-06,  2.9802e-05,  3.7551e-06,  ...,  1.0669e-05,\n",
      "         -7.4625e-05, -1.1623e-04],\n",
      "        [-8.6427e-06, -6.2346e-05,  6.6161e-06,  ..., -1.4424e-05,\n",
      "          3.3379e-06,  4.0531e-05],\n",
      "        [ 7.9691e-05,  7.2122e-05, -8.0705e-05,  ...,  8.4937e-05,\n",
      "          3.5703e-05, -1.3125e-04],\n",
      "        ...,\n",
      "        [ 1.1917e-02, -2.4014e-03,  9.2010e-03,  ...,  5.6887e-04,\n",
      "          9.8190e-03,  2.8076e-02],\n",
      "        [ 9.9716e-03,  8.3733e-04, -7.1335e-03,  ...,  5.9624e-03,\n",
      "         -1.0017e-02, -3.1891e-02],\n",
      "        [-3.3703e-03,  1.5211e-04, -1.5358e-02,  ...,  8.6746e-03,\n",
      "         -1.8875e-02, -5.7526e-02]], device='cuda:0', dtype=torch.float16)\n",
      "Update for parameter base_model.model.transformer.encoder.layers.25.self_attention.query_key_value.lora_B.default.weight: tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00],\n",
      "        [-0.0000e+00, -0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00, -0.0000e+00],\n",
      "        ...,\n",
      "        [ 0.0000e+00, -0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ...,  0.0000e+00,\n",
      "         -0.0000e+00, -5.9605e-08],\n",
      "        [-0.0000e+00,  0.0000e+00, -0.0000e+00,  ...,  0.0000e+00,\n",
      "         -0.0000e+00, -5.9605e-08]], device='cuda:0', dtype=torch.float16)\n",
      "Gradient of parameter base_model.model.transformer.encoder.layers.26.self_attention.query_key_value.lora_A.default.weight: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16)\n",
      "Update for parameter base_model.model.transformer.encoder.layers.26.self_attention.query_key_value.lora_A.default.weight: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16)\n",
      "Gradient of parameter base_model.model.transformer.encoder.layers.26.self_attention.query_key_value.lora_B.default.weight: tensor([[ 1.0233e-03,  6.2227e-04, -3.3617e-05,  ...,  1.3614e-04,\n",
      "          1.2426e-03, -4.1699e-04],\n",
      "        [-3.8290e-04, -2.2745e-04,  3.8981e-05,  ..., -4.9293e-05,\n",
      "         -4.5943e-04,  1.7726e-04],\n",
      "        [ 2.5654e-04,  1.4961e-04, -4.1187e-05,  ...,  4.2856e-05,\n",
      "          3.6430e-04, -1.3030e-04],\n",
      "        ...,\n",
      "        [-1.2276e-02, -1.2924e-02, -9.8495e-03,  ..., -4.4586e-02,\n",
      "          4.8904e-03, -7.2813e-04],\n",
      "        [-4.5227e-02, -4.5990e-02, -3.2654e-02,  ..., -8.1055e-02,\n",
      "         -3.7994e-02,  1.3359e-02],\n",
      "        [-4.4037e-02, -4.2725e-02, -3.1311e-02,  ..., -8.3008e-02,\n",
      "         -2.6337e-02,  1.0033e-02]], device='cuda:0', dtype=torch.float16)\n",
      "Update for parameter base_model.model.transformer.encoder.layers.26.self_attention.query_key_value.lora_B.default.weight: tensor([[ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00, -0.0000e+00],\n",
      "        [-0.0000e+00, -0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
      "         -0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00, -0.0000e+00],\n",
      "        ...,\n",
      "        [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -5.9605e-08,\n",
      "          0.0000e+00, -0.0000e+00],\n",
      "        [-5.9605e-08, -5.9605e-08, -5.9605e-08,  ..., -5.9605e-08,\n",
      "         -5.9605e-08,  0.0000e+00],\n",
      "        [-5.9605e-08, -5.9605e-08, -5.9605e-08,  ..., -5.9605e-08,\n",
      "         -0.0000e+00,  0.0000e+00]], device='cuda:0', dtype=torch.float16)\n",
      "Gradient of parameter base_model.model.transformer.encoder.layers.27.self_attention.query_key_value.lora_A.default.weight: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16)\n",
      "Update for parameter base_model.model.transformer.encoder.layers.27.self_attention.query_key_value.lora_A.default.weight: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16)\n",
      "Gradient of parameter base_model.model.transformer.encoder.layers.27.self_attention.query_key_value.lora_B.default.weight: tensor([[-4.3488e-04, -2.6345e-04, -2.5272e-04,  ...,  5.3167e-05,\n",
      "          2.9087e-05,  3.9101e-04],\n",
      "        [ 1.1081e-04,  9.1195e-05,  6.7055e-05,  ...,  1.9252e-05,\n",
      "         -1.7881e-07,  7.5638e-05],\n",
      "        [-2.2435e-04, -1.8048e-04, -8.7619e-05,  ..., -5.5075e-05,\n",
      "         -9.6917e-05, -2.4652e-04],\n",
      "        ...,\n",
      "        [-1.5884e-02, -5.8708e-03, -2.1706e-03,  ..., -9.2163e-03,\n",
      "         -1.4366e-02, -1.7035e-04],\n",
      "        [ 1.2352e-02,  6.4697e-03,  6.9580e-03,  ...,  2.1496e-03,\n",
      "          1.0376e-02, -3.4809e-03],\n",
      "        [-3.2654e-02, -6.8512e-03, -1.2840e-02,  ..., -1.5511e-02,\n",
      "         -2.1255e-02,  2.1133e-02]], device='cuda:0', dtype=torch.float16)\n",
      "Update for parameter base_model.model.transformer.encoder.layers.27.self_attention.query_key_value.lora_B.default.weight: tensor([[-0.0000e+00, -0.0000e+00, -0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "         -0.0000e+00,  0.0000e+00],\n",
      "        [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00],\n",
      "        ...,\n",
      "        [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00, -0.0000e+00],\n",
      "        [-5.9605e-08, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
      "         -0.0000e+00,  0.0000e+00]], device='cuda:0', dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "# 测试：到底为什么有nan（NON-USE）\n",
    "def print_parameter_updates(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            # 打印参数的梯度\n",
    "            print(f\"Gradient of parameter {name}: {param.grad}\")\n",
    "            # 打印参数更新量\n",
    "            update = param.grad * optimizer.param_groups[0]['lr']\n",
    "            print(f\"Update for parameter {name}: {update}\")\n",
    "\n",
    "print_parameter_updates(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试：到底为什么有nan（NON-USE）\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No NaN values found in model parameters.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 检查nan并找出哪个参数（NON-USE）\n",
    "import torch\n",
    "\n",
    "def check_for_nan(model):\n",
    "    nan_found = False\n",
    "    for name, param in model.named_parameters():\n",
    "        if torch.isnan(param).any():\n",
    "            print(f\"NaN detected in parameter: {name}\")\n",
    "            nan_found = True\n",
    "    if not nan_found:\n",
    "        print(\"No NaN values found in model parameters.\")\n",
    "    return nan_found\n",
    "\n",
    "check_for_nan(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_count = 1\n",
    "total_epoch = 50\n",
    "while (step_count <= total_epoch):\n",
    "    print(f\" ===Training Epoch {step_count}=== \")\n",
    "    start_time = time.time()\n",
    "    train_output = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        position_ids=position_ids,\n",
    "        labels=labels,\n",
    "    )\n",
    "    loss = train_output.loss\n",
    "    print(f\"loss:{loss}\")\n",
    "    if torch.isnan(loss):\n",
    "        print(\"nan detected\")\n",
    "        break\n",
    "        \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    end_time = time.time()\n",
    "    print(f\"time used:{end_time-start_time}\")\n",
    "\n",
    "    model_output_dir = \"LoRA_pretrained\"\n",
    "    print(\" ===Save the model=== \")\n",
    "    print(f\"the output dir is {model_output_dir}\")\n",
    "    model.save_pretrained(model_output_dir)\n",
    "    # LoRA will automatically save\n",
    "    \n",
    "    print(\" ===Save the optimizer=== \")\n",
    "    import os\n",
    "    optimizer_state_path = os.path.join(model_output_dir, \"optimizer.model\")\n",
    "    print(f\"the output path is {optimizer_state_path}\")\n",
    "    torch.save(optimizer.state_dict(), optimizer_state_path)\n",
    "    \n",
    "    step_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
